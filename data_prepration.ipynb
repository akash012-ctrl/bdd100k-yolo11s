{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SOURCE_IMAGES_DIR = r'd:\\bdd100k\\bdd100k_dataset\\bdd100k_images\\train'\n",
    "SOURCE_LABELS_DIR = r'd:\\bdd100k\\bdd100k_dataset\\bdd100k_labels\\train'\n",
    "DEST_DIR = r'd:\\bdd100k\\yolov11_dataset'\n",
    "\n",
    "\n",
    "CLASS_MAPPING = {\n",
    "    'car': 0,\n",
    "    'person': 1,\n",
    "    'truck': 2,\n",
    "    'motor': 3,\n",
    "    'rider': 4,\n",
    "    'bus': 5,\n",
    "    'train': 6\n",
    "}\n",
    "\n",
    "\n",
    "IMG_WIDTH = 1280\n",
    "IMG_HEIGHT = 720"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ab95f",
   "metadata": {},
   "source": [
    "## 1. Load and Filter Data\n",
    "We load the JSON labels to extract attributes (Weather, Time of Day, Scene) for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6974ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 70000 files in d:\\bdd100k\\bdd100k_dataset\\bdd100k_labels\\train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [13:45<00:00, 84.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 70000 images.\n",
      "                file_name             image_name weather  timeofday  \\\n",
      "0  0000f77c-6257be58.json  0000f77c-6257be58.jpg   clear    daytime   \n",
      "1  0000f77c-62c2a288.json  0000f77c-62c2a288.jpg   clear  dawn/dusk   \n",
      "2  0000f77c-cb820c98.json  0000f77c-cb820c98.jpg   clear  dawn/dusk   \n",
      "3  0001542f-5ce3cf52.json  0001542f-5ce3cf52.jpg   clear      night   \n",
      "4  0001542f-7c670be8.json  0001542f-7c670be8.jpg   clear      night   \n",
      "\n",
      "         scene  \n",
      "0  city street  \n",
      "1      highway  \n",
      "2  residential  \n",
      "3  city street  \n",
      "4      highway  \n"
     ]
    }
   ],
   "source": [
    "def load_metadata(label_dir):\n",
    "    \"\"\"\n",
    "    Load metadata from JSON files for sampling.\n",
    "    Returns a DataFrame with file_name, weather, timeofday, scene.\n",
    "    \"\"\"\n",
    "    metadata = []\n",
    "    json_files = [f for f in os.listdir(label_dir) if f.endswith('.json')]\n",
    "    \n",
    "    print(f\"Scanning {len(json_files)} files in {label_dir}...\")\n",
    "    \n",
    "    for filename in tqdm(json_files):\n",
    "        filepath = os.path.join(label_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # BDD100K json structure check\n",
    "                attrs = data.get('attributes', {})\n",
    "                \n",
    "                # Handle image name: ensure it has .jpg extension\n",
    "                image_name = data.get('name', filename.replace('.json', '.jpg'))\n",
    "                if not image_name.endswith('.jpg'):\n",
    "                    image_name += '.jpg'\n",
    "                    \n",
    "                metadata.append({\n",
    "                    'file_name': filename, # JSON filename\n",
    "                    'image_name': image_name,\n",
    "                    'weather': attrs.get('weather', 'unknown'),\n",
    "                    'timeofday': attrs.get('timeofday', 'unknown'),\n",
    "                    'scene': attrs.get('scene', 'unknown')\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "            \n",
    "    return pd.DataFrame(metadata)\n",
    "\n",
    "# Load metadata\n",
    "df_metadata = load_metadata(SOURCE_LABELS_DIR)\n",
    "print(f\"Loaded metadata for {len(df_metadata)} images.\")\n",
    "print(df_metadata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d97188",
   "metadata": {},
   "source": [
    "##### 2. Data Sampling \n",
    "1.  Filter for weather == 'clear'.\n",
    "2.  Stratify by timeofday:\n",
    "    *   Daytime: ~2,500 (50%)\n",
    "    *   Night: ~2,000 (40%)\n",
    "    *   Dawn/Dusk: ~500 (10%)\n",
    "3.  Total: 5,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77122f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'clear' weather images: 37411\n",
      "Available Daytime: 12477\n",
      "Available Night: 22928\n",
      "Available Dawn/Dusk: 2004\n",
      "Selected 5000 images for the dataset.\n",
      "timeofday\n",
      "daytime      2500\n",
      "night        2000\n",
      "dawn/dusk     500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Filter for Clear Weather\n",
    "df_clear = df_metadata[df_metadata['weather'] == 'clear'].copy()\n",
    "print(f\"Total 'clear' weather images: {len(df_clear)}\")\n",
    "\n",
    "\n",
    "# Targets\n",
    "TARGET_TOTAL = 5000\n",
    "TARGET_DAY = 2500\n",
    "TARGET_NIGHT = 2000\n",
    "TARGET_DAWN_DUSK = 500\n",
    "\n",
    "# Separate by time\n",
    "df_day = df_clear[df_clear['timeofday'] == 'daytime']\n",
    "df_night = df_clear[df_clear['timeofday'] == 'night']\n",
    "df_dawn_dusk = df_clear[df_clear['timeofday'].isin(['dawn/dusk'])]\n",
    "\n",
    "# Check availability\n",
    "print(f\"Available Daytime: {len(df_day)}\")\n",
    "print(f\"Available Night: {len(df_night)}\")\n",
    "print(f\"Available Dawn/Dusk: {len(df_dawn_dusk)}\")\n",
    "\n",
    "# Sample\n",
    "# Use min() to avoid error if not enough data\n",
    "sample_day = df_day.sample(n=min(len(df_day), TARGET_DAY), random_state=42)\n",
    "sample_night = df_night.sample(n=min(len(df_night), TARGET_NIGHT), random_state=42)\n",
    "sample_dawn_dusk = df_dawn_dusk.sample(n=min(len(df_dawn_dusk), TARGET_DAWN_DUSK), random_state=42)\n",
    "\n",
    "# Combine\n",
    "df_sampled = pd.concat([sample_day, sample_night, sample_dawn_dusk])\n",
    "print(f\"Selected {len(df_sampled)} images for the dataset.\")\n",
    "print(df_sampled['timeofday'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6c8cb",
   "metadata": {},
   "source": [
    "##### 3. Split Dataset\n",
    "Split the 5,000 images into:\n",
    "*   Train: 70%\n",
    "*   Test: 20%\n",
    "*   Validation: 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f540ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3500\n",
      "Test set size: 1000\n",
      "Val set size: 500\n"
     ]
    }
   ],
   "source": [
    "# First split: Train (70%) vs Temp (30%)\n",
    "train_df, temp_df = train_test_split(df_sampled, test_size=0.3, random_state=42, stratify=df_sampled['timeofday'])\n",
    "\n",
    "# Second split: Temp into Test (20% of total -> 2/3 of Temp) and Val (10% of total -> 1/3 of Temp)\n",
    "test_df, val_df = train_test_split(temp_df, test_size=1/3, random_state=42, stratify=temp_df['timeofday'])\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Val set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e229f",
   "metadata": {},
   "source": [
    "##### 4. Create Directory Structure and Process Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f86a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train set (3500 images)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [00:49<00:00, 71.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing valid set (500 images)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:07<00:00, 70.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set (1000 images)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:13<00:00, 72.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset preparation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_yolo_label(json_path, class_mapping, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Reads a BDD100K JSON label file and converts it to YOLO format string.\n",
    "    \"\"\"\n",
    "    yolo_lines = []\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        frames = data.get('frames', [])\n",
    "        if not frames:\n",
    "            return []\n",
    "            \n",
    "        # BDD100K usually has objects in the first frame for single image datasets\n",
    "        objects = frames[0].get('objects', [])\n",
    "        \n",
    "        for obj in objects:\n",
    "            category = obj.get('category')\n",
    "            \n",
    "            if category in class_mapping:\n",
    "                class_id = class_mapping[category]\n",
    "                \n",
    "                # Get box2d\n",
    "                box2d = obj.get('box2d')\n",
    "                if box2d:\n",
    "                    x1 = box2d['x1']\n",
    "                    y1 = box2d['y1']\n",
    "                    x2 = box2d['x2']\n",
    "                    y2 = box2d['y2']\n",
    "                    \n",
    "                    # Convert to YOLO format (center_x, center_y, width, height) normalized\n",
    "                    # Ensure coordinates are within image bounds\n",
    "                    x1 = max(0, min(x1, img_width))\n",
    "                    x2 = max(0, min(x2, img_width))\n",
    "                    y1 = max(0, min(y1, img_height))\n",
    "                    y2 = max(0, min(y2, img_height))\n",
    "                    \n",
    "                    bw = x2 - x1\n",
    "                    bh = y2 - y1\n",
    "                    \n",
    "                    if bw <= 0 or bh <= 0:\n",
    "                        continue\n",
    "                        \n",
    "                    bx = (x1 + x2) / 2.0\n",
    "                    by = (y1 + y2) / 2.0\n",
    "                    \n",
    "                    # Normalize\n",
    "                    nw = bw / img_width\n",
    "                    nh = bh / img_height\n",
    "                    nx = bx / img_width\n",
    "                    ny = by / img_height\n",
    "                    \n",
    "                    yolo_lines.append(f\"{class_id} {nx:.6f} {ny:.6f} {nw:.6f} {nh:.6f}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing label {json_path}: {e}\")\n",
    "        return []\n",
    "        \n",
    "    return yolo_lines\n",
    "\n",
    "def process_dataset(df, split_name):\n",
    "    \"\"\"\n",
    "    Process a dataframe of images: copy image, create label file.\n",
    "    split_name: 'train', 'valid', or 'test'\n",
    "    \"\"\"\n",
    "    print(f\"Processing {split_name} set ({len(df)} images)...\")\n",
    "    \n",
    "    # Create directories with the requested structure:\n",
    "    # dataset/\n",
    "    # ├── train/\n",
    "    # │   ├── images/\n",
    "    # │   └── labels/\n",
    "    \n",
    "    split_dir = os.path.join(DEST_DIR, split_name)\n",
    "    img_dest_dir = os.path.join(split_dir, 'images')\n",
    "    lbl_dest_dir = os.path.join(split_dir, 'labels')\n",
    "    \n",
    "    os.makedirs(img_dest_dir, exist_ok=True)\n",
    "    os.makedirs(lbl_dest_dir, exist_ok=True)\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        json_filename = row['file_name']\n",
    "        image_filename = row['image_name']\n",
    "        \n",
    "        # Source paths\n",
    "        json_path = os.path.join(SOURCE_LABELS_DIR, json_filename)\n",
    "        img_path = os.path.join(SOURCE_IMAGES_DIR, image_filename)\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(img_path):\n",
    "            # Try to find the image if the extension is different or missing\n",
    "            if not image_filename.endswith('.jpg'):\n",
    "                 image_filename = image_filename.replace('.json', '.jpg')\n",
    "                 img_path = os.path.join(SOURCE_IMAGES_DIR, image_filename)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: Image not found {img_path}\")\n",
    "                continue\n",
    "        \n",
    "        # Generate YOLO label content\n",
    "        yolo_lines = create_yolo_label(json_path, CLASS_MAPPING, IMG_WIDTH, IMG_HEIGHT)\n",
    "        \n",
    "        # Destination paths\n",
    "        dest_img_path = os.path.join(img_dest_dir, image_filename)\n",
    "        dest_lbl_path = os.path.join(lbl_dest_dir, image_filename.replace('.jpg', '.txt'))\n",
    "        \n",
    "        # Copy Image\n",
    "        try:\n",
    "            shutil.copy2(img_path, dest_img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to copy image {img_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Write Label\n",
    "        try:\n",
    "            with open(dest_lbl_path, 'w') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write label {dest_lbl_path}: {e}\")\n",
    "\n",
    "# Execute processing with the requested split names\n",
    "# Note: User requested 'valid' for validation set\n",
    "process_dataset(train_df, 'train')\n",
    "process_dataset(val_df, 'valid')\n",
    "process_dataset(test_df, 'test')\n",
    "\n",
    "print(\"\\nDataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ece79",
   "metadata": {},
   "source": [
    "## 5. Create data.yaml\n",
    "Create the `data.yaml` file required by YOLOv11 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data.yaml at d:\\bdd100k\\yolov11_dataset\\data.yaml\n",
      "\n",
      "path: d:\\bdd100k\\yolov11_dataset # dataset root dir\n",
      "train: train/images # train images (relative to 'path')\n",
      "val: valid/images # val images (relative to 'path')\n",
      "test: test/images # test images (optional)\n",
      "\n",
      "nc: 7 # number of classes\n",
      "names: ['car', 'person', 'truck', 'motor', 'rider', 'bus', 'train'] # class names\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yaml_content = f\"\"\"\n",
    "path: {DEST_DIR} # dataset root dir\n",
    "train: train/images # train images (relative to 'path')\n",
    "val: valid/images # val images (relative to 'path')\n",
    "test: test/images # test images (optional)\n",
    "\n",
    "nc: {len(CLASS_MAPPING)} # number of classes\n",
    "names: {['car', 'person', 'truck', 'motor', 'rider', 'bus', 'train']} \n",
    "\"\"\"\n",
    "\n",
    "yaml_path = os.path.join(DEST_DIR, 'data.yaml')\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"Created data.yaml at {yaml_path}\")\n",
    "print(yaml_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
